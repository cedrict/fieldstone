\noindent
\includegraphics[height=1.25cm]{images/pictograms/benchmark}
\includegraphics[height=1.25cm]{images/pictograms/under_construction}
\includegraphics[height=1.25cm]{images/pictograms/FEM}
\includegraphics[height=1.25cm]{images/pictograms/paraview}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{flushright} {\tiny {\color{gray} python\_codes/fieldstone\_147/text.tex}} \end{flushright}

%\lstinputlisting[language=bash,basicstyle=\small]{python_codes/fieldstone_147/keywords.key}

\par\noindent\rule{\textwidth}{0.4pt}

\begin{center}
\inpython
{\small Code: \url{https://github.com/cedrict/fieldstone/tree/master/python_codes/fieldstone_147}}
\end{center}

\par\noindent\rule{\textwidth}{0.4pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The idea behind this \stone is the comparison of various approaches to solve the 
Stokes system. We here consider simple setups in a 2D rectangular domain. 
The code relies on the \QtwoQone element pair and is based on \stone~\ref{f18}.

In almost all previous stones I have used \lstinline{scipy.sparse.linalg.spsolve} 
to solve the Stokes linear system but looking online we find that there are many more methods 
available\footnote{\url{https://docs.scipy.org/doc/scipy/reference/sparse.linalg.html}}.

As far as I know there are 3 different approaches to solve the Stokes system:

\begin{itemize}
\item Direct methods for linear equation systems
\begin{itemize}
\item \lstinline{spsolve}: It looks like it is the SuperLU 
solver\footnote{\url{https://caam37830.github.io/book/02_linear_algebra/sparse_linalg.html}}:  (\lstinline{solver=1}) 
\item \lstinline{spsolve+umfpack}: (\lstinline{solver=11}) 
\end{itemize}
\item Iterative methods for linear equation systems\footnote{
If we build the system as a single (sparse) two-dimensional array 
we can right away rule out \lstinline{cg} which requires 
a SPD matrix, see \url{https://en.wikipedia.org/wiki/Conjugate_gradient_method}.}:
\begin{itemize}
\item \lstinline{bicg(A, b[, x0, tol, maxiter, M, callback, atol])} - (\lstinline{solver=9})
\item \lstinline{bicgstab}(A, b[, x0, tol, maxiter, M, ...])        - (\lstinline{solver=10})
\item \lstinline{cgs}(A, b[, x0, tol, maxiter, M, callback, atol])  - (\lstinline{solver=12})
\item \lstinline{gmres}(A, b[, x0, tol, restart, maxiter, M, ...])  - (\lstinline{solver=2})
\item \lstinline{lgmres}(A, b[, x0, tol, maxiter, M, ...])          - (\lstinline{solver=3})
\item \lstinline{minres}(A, b[, x0, shift, tol, maxiter, M, ...])   - (\lstinline{solver=5})
\item \lstinline{qmr}(A, b[, x0, tol, maxiter, M1, M2, ...])        - (\lstinline{solver=6})
\item \lstinline{gcrotmk}(A, b[, x0, tol, maxiter, M, ...])         - (\lstinline{solver=8})
\item \lstinline{tfqmr}(A, b[, x0, tol, maxiter, M, callback, ...]) - (\lstinline{solver=7})
\end{itemize}
\item Iterative methods applied to the Schur complement, with or without L2
projection presented in \textcite{jalt26}. These are typically called the 
Uzawa1, Uzawa2 and Uzawa3 method, the latter corresponding to the Conjugate Gradient approach.
This requires the $\K$ and $\G$ blocks to be built separately as explained in Section~\ref{MMM-ss:schurpcg}.
For historical reasons, there are here both a Uzawa3 {\tt uzawa3\_solver.py} implementation and 
a {\tt schur\_complement\_cg\_solver.py} which predates it.
All these methods require an inner solver called once per iteration on the $\K$ block.
We then have the choice between using a direct solver (spsolve \lstinline{solver=4}) or using a conjugate gradient
approach since $\K$ is SPD (\lstinline{solve=13}). 
By using CG for the inner solve we pretty much ensure that memory requirements remain very low.
Another variant consists in acknowledging that inside the iterations the 
block $\K$ does not change, so that we can compute its LU decomposition
and use this decomposition every time that we need to solve a linear
system with $\K$. This is \lstinline{solver=14} and \lstinline{solver=23}. 
This option means that the LU factors are stored in the memory longer than in the standard
case but with equal memory use.

\end{itemize}

All these methods should also be used with a preconditioner, which we will investigate later on.

Iterative solvers require a tolerance value and it is set to $10^{-7}$ unless mentioned otherwise.

Note that the \aspect solver presented in \textcite{krhb12} (2012) is rather complex. It is 
based on FGMRES and clever preconditioners. It will not be explored here.

Feeding the full Stokes matrix to a bunch of iterative solvers without thinking much 
about it is rather naive.




%===========================================================
\section*{The list of implemented solver strategies}

\begin{enumerate}

\item[solver=1] (Full Stokes Matrix) the usual one I use in all the stones (i.e. direct solver on Stokes matrix)
\begin{lstlisting}
   sol=sps.linalg.spsolve(sparse_matrix,rhs,use_umfpack=False)
\end{lstlisting}

\item[solver=2] (Full Stokes Matrix) the gmres from linalg 
\begin{lstlisting}
   sol,info=sps.linalg.gmres(sparse_matrix,b_fem,restart=2000,tol=tolerance)
\end{lstlisting}

\item[solver=3]  (Full Stokes Matrix) lgmres from linalg
\begin{lstlisting}
   sol=sps.linalg.lgmres(sparse_matrix,b_fem,atol=1e-16,tol=tolerance)[0]
\end{lstlisting}

\item[solver=4] (Schur complement approach) my Schur complement CG solver using direct solver for inner solve 
on $\K$ matrix
\begin{lstlisting}
   solV,p,niter=schur_complement_cg_solver(K_mat,G_mat,M_mat,f_rhs,h_rhs,\
                                          NfemV,NfemP,niter_max,tolerance,use_precond,'direct')
\end{lstlisting}

\item[solver=5] (Full Stokes Matrix) minres from linalg
\begin{lstlisting}
   sol=sps.linalg.minres(sparse_matrix,b_fem, tol=1e-10)[0]
\end{lstlisting}

\item[solver=6] (Full Stokes Matrix) qmr from linalg
\begin{lstlisting}
   sol=sps.linalg.qmr(sparse_matrix,b_fem,tol=tolerance)[0]
\end{lstlisting}

\item[solver=7]  (Full Stokes Matrix) tfqmr from linalg
\begin{lstlisting}
   sol=sps.linalg.tfqmr(sparse_matrix,b_fem, tol=1e-10)[0]
\end{lstlisting}

\item[solver 8]  (Full Stokes Matrix) gcrotmk from linalg
\begin{lstlisting}
   sol=sps.linalg.gcrotmk(sparse_matrix,b_fem,atol=1e-16, tol=1e-10)[0]
\end{lstlisting}

\item[solver 9] (Full Stokes Matrix) bicg from linalg
\begin{lstlisting}
   sol=sps.linalg.bicg(sparse_matrix,b_fem, tol=tolerance)[0]
\end{lstlisting}

\item[solver=10]  (Full Stokes Matrix) bicgstab from linalg
\begin{lstlisting}
   sol=sps.linalg.bicgstab(sparse_matrix,b_fem, tol=tolerance)[0]
\end{lstlisting}

\item[solver=11]  (Full Stokes Matrix) direct solver spsolve using umfpack 
\begin{lstlisting}
   sol=sps.linalg.spsolve(sparse_matrix,rhs,use_umfpack=True)
\end{lstlisting}

\item[solver=12]  (Full Stokes Matrix) cgs from linalg
\begin{lstlisting}
   sol=sps.linalg.cgs(sparse_matrix,b_fem, tol=tolerance)[0]
\end{lstlisting}

\item[solver=13]  (Schur complement approach) my Schur complement CG solver using cg for inner solve
\begin{lstlisting}
   solV,p,niter=schur_complement_cg_solver(K_mat,G_mat,M_mat,f_rhs,h_rhs,\
                                           NfemV,NfemP,niter_max,tolerance,use_precond,'cg')
\end{lstlisting}

\item[solver=14]  (Schur complement approach) my Schur complement CG solver using splu for inner solve
\begin{lstlisting}
   solV,p,niter=schur_complement_cg_solver(K_mat,G_mat,M_mat,f_rhs,h_rhs,\
                                           NfemV,NfemP,niter_max,tolerance,use_precond,'splu')
\end{lstlisting}

\item[solver=15]  (Schur complement approach) uzawa 1: Section 5.1 \cite{braess}. Parameterised by $\omega$
\begin{lstlisting}
   solV,p,niter=uzawa1_solver(K_mat,G_mat,M_mat,f_rhs,h_rhs,\
                       NfemV,NfemP,niter_max,tolerance,use_precond,'direct',omega)
\end{lstlisting}


\item[solver=16] (Schur complement approach) uzawa 2: Section 5.2 \cite{braess}. 
\begin{lstlisting}
   solV,p,niter=uzawa2_solver(K_mat,G_mat,M_mat,f_rhs,h_rhs,\
                       NfemV,NfemP,niter_max,tolerance,use_precond,'direct')
\end{lstlisting}


\item[solver=17] (Schur complement approach) projection solver {\color{red} where does it come from?!} 
\begin{lstlisting}
solV,p,niter=projection_solver(K_mat,G_mat,L_mat,f_rhs,h_rhs,NfemV,NfemP,niter_max,tolerance)
\end{lstlisting}


\item[solver=18]  (Schur complement approach) Uzawa 1 + L2 projection
\begin{lstlisting}
solV,p,niter=uzawa1_solver_L2(K_mat,G_mat,MP_mat,f_rhs,h_rhs,NfemP,niter_max,tolerance,omega)
\end{lstlisting}

\item[solver=19] (Schur complement approach) Uzawa 1b + L2 projection
\begin{lstlisting}
solV,p,niter=uzawa1_solver_L2b(K_mat,G_mat,MP_mat,f_rhs,h_rhs,NfemP,niter_max,tolerance,omega)
\end{lstlisting}

\item[solver=20] (Schur complement approach) Uzawa 2 + L2 projection
\begin{lstlisting}
solV,p,niter=uzawa2_solver_L2(K_mat,G_mat,MP_mat,H_mat,f_rhs,h_rhs,NfemP,niter_max,tolerance)
\end{lstlisting}

\item[solver=21] (Schur complement approach) Uzawa 3 (CG on Schur) projection
\begin{lstlisting}
solV,p,niter=uzawa3_solver(K_mat,G_mat,f_rhs,h_rhs,NfemP,niter_max,tolerance)
\end{lstlisting}

\item[solver=22] (Schur complement approach) Uzawa 3 (CG on Schur) + L2 projection
\begin{lstlisting}
solV,p,niter=uzawa3_solver_L2(K_mat,G_mat,MP_mat,H_mat,f_rhs,h_rhs,NfemP,niter_max,tolerance,'direct')
\end{lstlisting}

\item[solver=23]  (Schur complement approach)  Uzawa 3 (CG on Schur), LU inner, L2 projection
\begin{lstlisting}
solV,p,niter=uzawa3_solver_L2(K_mat,G_mat,MP_mat,H_mat,f_rhs,h_rhs,NfemP,niter_max,tolerance,'splu')
\end{lstlisting}

It relies on {\tt scipy.sparse.linalg.splu} which computes the LU decomposition of a sparse (CSC), square matrix,
see documentation at \url{https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.splu.html}.


\end{enumerate}

A few remarks:

\begin{itemize}
\item All the solvers above containing the ``L2 projection'' (18 to 23) are related to the 
article \textcite{jalt26}. They were written specifically in the context of this publication 
and obviously the Uzawa3 of solver 21 is identical in theory to solver 4. 
More information about the L2 stuff is presented hereafter, see `About the Uzawa solvers' 
and `L2 projection stuff' sections.


\item this \stone is not optimised for performance: the assembly and matrix storage could 
be improved.

\item Numba is not used 

\item About solver 13: in the earlier versions of the {\tt schur\_complement\_cg\_solver.py} 
the rhs passed to {\tt sps.linalg.cg} inside the outer iterations gradually became  
smaller and smaller in magnitude, to the point that it became smaller than the tolerance 
of the cg solver and the returned solution was then zero, which derailed the outer solver.
A quick fix is now implemented: the rhs is scaled with its maximum value and the returned 
solution is scaled back:
\begin{lstlisting}
for k in range (0,niter):
    ...
    rhsmax=np.max(ptildevect_k)
    dvect_k=sps.linalg.cg(K_mat,ptildevect_k/rhsmax)[0]
    dvect_k*=rhsmax
\end{lstlisting}


\end{itemize}

\newpage
%====================================================
\section*{Donea \& Huerta manufactured solution}

We start by running the code on all 23 solvers without any preconditioner. 
We set the maximum number of iterations to 1000 (when relevant) and the tolerance to $10^{-7}$.
We run {\tt script\_test\_all\_solvers1} at a $16\times 16$ resolution:
\begin{center}
\includegraphics[width=5.7cm]{python_codes/fieldstone_147/RESULTS/simpletest/16/errv.pdf}
\includegraphics[width=5.7cm]{python_codes/fieldstone_147/RESULTS/simpletest/16/errp.pdf}
\includegraphics[width=5.7cm]{python_codes/fieldstone_147/RESULTS/simpletest/16/divv.pdf}\\
{\captionfont Results and plots are in {\tt python\_codes/fieldstone\_147/RESULTS/simpletest}.}
\end{center}
and at $32\times 32$ resolution:
\begin{center}
\includegraphics[width=5.7cm]{python_codes/fieldstone_147/RESULTS/simpletest/32/errv.pdf}
\includegraphics[width=5.7cm]{python_codes/fieldstone_147/RESULTS/simpletest/32/errp.pdf}
\includegraphics[width=5.7cm]{python_codes/fieldstone_147/RESULTS/simpletest/32/divv.pdf}\\
{\captionfont Results and plots are in {\tt python\_codes/fieldstone\_147/RESULTS/simpletest}.}
\end{center}

Conclusions so far:

\begin{center}
\begin{tabular}{p{2cm}|p{1cm}p{12cm}}
\hline
solver 1  & ok & \\
solver 2  & ok & \\
solver 3  & ok & \\
solver 4  & ok & \\
solver 5  & X & yields good results with $tol=10^{-12}$\\
solver 6  & ok & \\
solver 7  & X & fails to yield good results, even with $tol=10^{-12}$ \\
solver 8  & ok & \\
solver 9  & ok & \\
solver 10 & X & completely explodes\\
solver 11 & ok & \\
solver 12 & X & completely explodes\\
solver 13 & ok & \\
solver 14 & ok & \\
solver 15 & X & This was for $\omega=1$. Tests show that $\omega=1000$ yield good results. \\
solver 16 & ok & \\
solver 17 & X  & fails to converge - extremely low conv rate\\
solver 18 & ok & $\omega=1$\\
solver 19 & ok & $\omega=1$\\
solver 20 & ok & \\
solver 21 & ok & \\
solver 22 & ok & \\
solver 23 & ok & \\
\hline
\end{tabular}
\end{center}

We then discard solvers 7, 10, 12, 17 and run now the remaining ones on 48x48 mesh using
the {\tt script\_test\_all\_solvers2} script:

\begin{center}
\includegraphics[width=5.7cm]{python_codes/fieldstone_147/RESULTS/simpletest/48/errv.pdf}
\includegraphics[width=5.7cm]{python_codes/fieldstone_147/RESULTS/simpletest/48/errp.pdf}
\includegraphics[width=5.7cm]{python_codes/fieldstone_147/RESULTS/simpletest/48/divv.pdf}
\end{center}

We find that the remaining solvers all yield very similar results.
However we also find that solver 15 (Uzawa 1) is {\it very} slow and since it 
requires a user-chosen parameter we will also discard it in what follows
(in the case above it did carry out 500 iterations but had not reached the tolerance yet).

In the end remain: 
{\tt 1 2 3 4 5 6 8 9 11 13 14 16 18 19 20 21 22 23}.

\newpage

I then proceed to run these solvers over a range of resolutions, i.e. $8^2$ to $64^2$ elements.
I then monitor the discretization errors (indicative of the accuracy of the solution),
the time per outer iteration (when applicable) and the total solve time:

\begin{center}
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/timings/errv.pdf}
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/timings/errp.pdf}\\
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/timings/solve_time.pdf}
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/timings/solve_time2.pdf}\\
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/timings/solve_time3.pdf}
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/timings/time_per_iteration.pdf}\\
{\captionfont Results obtained with {\tt script\_errors}.}
\end{center}

Remark: 

\begin{itemize}
\item iteration time is very low for solver 23 (uzawa3 + LU inner) because the LU
decomposition is carried out outside of the iterations.

%\item solver 13 fails to converge (schur complement CG + cg inner), since the rhs 
%of the inner solve becomes very small and yields a zero solution for the inner solver. 

\item Solvers 1, 11, 14, 23 are the fastest. 1 and 11 are direct solvers, 
but relying on different functions/approaches, 
while both 14 and 23 both use LU decomposition.

\item most of the solvers used here tend to use all 8 cores on my laptop by default.

\end{itemize}

In what follows I focus on 1, 14 and 23 since 11 relies on umfpack which is not necessarily 
installed by default and also does not yield substantially different results than solver 1.

\newpage

Solvers 1, 14, 23 only:

\begin{center}
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/timings2/errv.pdf}
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/timings2/errp.pdf}\\
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/timings2/solver_convergence_14.pdf}
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/timings2/solver_convergence_23.pdf}\\
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/timings2/solve_time.pdf}
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/timings2/solve_time2.pdf}\\
\includegraphics[width=12cm]{python_codes/fieldstone_147/RESULTS/timings2/solve_time3.pdf}\\
{\captionfont Results obtained with {\tt script\_errors\_1\_14\_23}, in 
{\tt fieldstone\_147/RESULTS/timings}.}
\end{center}

The number of iterations in independent of resolution. 

It is different for 14 and 23. 14 does more iterations but they are cheaper. 

At resolution 192x192 solvers 14 \& 23 are approx. 3x faster than solver 1



%We start by looking at the velocity and pressure discretisation errors in the $L_2$ norm:
%\begin{center}
%\includegraphics[width=15cm]{python_codes/fieldstone_147/RESULTS/errors.pdf}
%\end{center}
%We find that most solvers yield the expected convergence rates (cubic for velocity,
%quadratic for pressure) although a few clearly 'derail' at higher resolutions (MINRES, LGMRES, GCROTMK).

%\newpage
%(OLD)We now turn to the time to solution measurements:
%\begin{center}
%\includegraphics[width=14cm]{python_codes/fieldstone_147/RESULTS/solve.pdf} \\
%\includegraphics[width=14cm]{python_codes/fieldstone_147/RESULTS/solve2.pdf}\\
%{\captionfont Time to solve linear system. Only results for usable solvers are shown. 
%Left: time as a function of the number of elements; Right: 
%time as a function of the total number of dofs.}
%\end{center}
%We find that these times can vary by more than one order of magnitude.

\newpage

\subsection*{A quick parameter study}

%We can also focus on the SC-CG solver and look at the required number of 
%outer iterations to reach the desired tolerance:
%\begin{center}
%\includegraphics[width=12cm]{python_codes/fieldstone_147/RESULTS/convergence_schur_cpl.pdf}\\
%{\captionfont Convergence of the SC-CG solver as function of resolution.}
%\end{center}

On the following figure I plot the velocity and pressure errors for SC-CG+ilu solver 
for different values of the relative tolerance. We find that the solve time is not 
much influenced by the tolerance, but unsurprisingly the accuracy of the solution is: 
\begin{center}
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/tol_study_solver14/errors.pdf}
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/tol_study_solver14/solve.pdf}\\
{\captionfont Left: Influence of the relative tolerance on the accuracy of the solution obtained 
with the SC-CG+ilu ({\python solver=14}). Right: solve time in seconds.}
\end{center}
It appears that a relative tolerance of $10^{-6}$ is enough, but I set it to $10^{-7}$ default
to be on the safe side (when looking closley at high resolution it looks like the 
blue curve of $10^{-6}$  seems to separate from the orange one from $10^{-7}$).

I have also run \aspect on the same problem (with the same type of elements) and on only 
1 core (while the spsolve inner solve tends to use all available cores of my laptop). 
Since \aspect reports the assembly time, the preconditioner time and the solve
time separately I add them together.
We find that the solver outperforms mine (obviously!) and more importantly that 
is scales linearly with the number of elements. 
The highest resolutions so far are $300\times 300$ elements and the memory requirements 
were about half of my laptop RAM, so about 16Gb, and $400\times 400$ which requires
about 20+ Gb.





\newpage
%......................................
\subsubsection*{About the Uzawa solvers}

This subsection was written and used to review an early version of \textcite{jalt26}.

U1 is uzawa of Braess, section 5.1: simple to implement but a parameter is needed. 
U2 is uzawa of Braess, section 5.2
U3 is conjugate gradient on schur complement, section 5.3.

The three algorithms U1, U2, U3 are reproduced hereunder from left to right for convenience:
\begin{center}
\fbox{\includegraphics[width=5.6cm]{python_codes/fieldstone_147/images/braess_U1}}
\fbox{\includegraphics[width=5.6cm]{python_codes/fieldstone_147/images/braess_U2}}
\fbox{\includegraphics[width=5.6cm]{python_codes/fieldstone_147/images/braess_U3}}\\
Taken from \textcite{braess}.
\end{center}

The following figure shows the convergence of this solver for different values of this parameter.
20000 makes the solver explode.	

\begin{center}
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/uzawa123/convergence_24x24.pdf}
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/uzawa123/convergence_48x48.pdf}\\
{\captionfont Convergence on 24x24 mesh (left) and 48x48 mesh (right) 
for Donea \& Huerta manufactured solution.}
\end{center}

We see that the $\alpha$ parameter should be a function of resolution, and that 
the U2 converges in the same number of iterations because it automatically 
computes the step.


Actually when focusing now on U2 and U3 only and running the experiment at increasing resolutions I find ({\it as expected for stable element pairs}) that the number of iterations is independent of the resolution:
\begin{center}
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/uzawa123/convergence_uzawa23}
\end{center}



This shows that
\begin{enumerate}
\item U2 is a much better/safe algorithm than U1 while being equally simple to implement, 
thereby proving that U1 should be discarded altogether; 
\item U3 is the obvious starting point if one wishes to offer an improvement on Uzawa-type 
solvers as it is by far the best of the three and is marginally more complex to implement than U1 or U2.
\end{enumerate}



%\newpage
%%-------------------------
%\subsection*{Conclusions}

%\begin{itemize}
%\item direct method spsolve about 10x faster than all others (except solver 14!)
%\item UMFPACK option in spsolve does not yield substantial differences
%\item my own Schur complement CG solver rivals other LINALG solvers (solver 4, 13) ! 
%\item MINRES, even with very low tolerance 1e-10, ultimately fails
%\item LGMRES seems to work fine, very similar to GMRES but does not always converge.
%\item TFQMR, BICGSTAB, CGS are out: they fail to converge and/or return 
%very wrong results
%\item GCROTMK usable but slowest, also fails at high resolution 
%\item number of iterations of SC-CG is independent of resolution, as expected
%from a stable element pair (for isoviscous flow)
%\item solve time for iterative method rather comparable, no clear best
%\item SC-CG with direct or cg inner solve are comparable in terms of 
%solve time. There is still something wrong about my having to 
%set tol of cg extremely low, and even then it ends up crashing at nel=52 ...
%\item my own SC-CG+ilu is faster than spsolve!
%\end{itemize}

%In the end, I keep SPSOLVE, SC-CG, GMRES, QMR, BICG.











\newpage
%==============================================================
\section*{L2 projection stuff - Donea \& Huerta}

In what follows we will need to replace $\vec{p}^T\cdot\vec{q}$ 
by 
\[
\langle p,q \rangle = \int_\Omega {p} \cdot {q} dV 
= \vec{p}^T \cdot {\bm M}_{p} \cdot \vec{q}
\]
where ${\bm M}_{p}$ is the pressure mass matrix. 
Building and re-using the pressure mass matrix is simpler than 
computing the integral everytime, but it may not be 
the fastest approach.

Likewise we will need to replace $( \G \cdot \vec{x})^T\cdot \vec{y}$
by
\[
\langle \nabla {x},{y} \rangle = \int_\Omega \nabla \vec{x} \; {y} dV = \vec{x}^T \cdot {\bm H} \cdot \vec{y}
\]
where ${\bm H}$ is defined as $\int \vec\nabla \vec{\bN}_P  \cdot \vec{\bN}_V dV$ (not exactly, see below). 

Note that in the case of this benchmark the velocity boundary conditions
are no slip on all sides, so that in practive $\vec{h}=0$.

%---------------------------------------------------
\subsection*{Uzawa 1 algorithm}

Assume $\vec{\cal P}_0$ known
\begin{eqnarray}
\text{solve} \qquad \mathbb{K} \cdot {\color{teal} \vec{\cal V}_k} 
&=& {\color{teal} \vec{f}} - \mathbb{G}\cdot {\color{carrotorange} \vec{\cal P}_{k}}  \nn\\
{\color{carrotorange} \vec{\cal P}_{k+1}} &=& 
{\color{carrotorange} \vec{\cal P}_{k}}  
+ \alpha_k (\mathbb{G}^T\cdot {\color{teal} \vec{\cal V}_k}   - {\color{carrotorange}\vec{h}})
\quad
\quad
\quad
\quad
k=0,1,2, ... 
\end{eqnarray}

Vectors this {\color{teal} color} are vectors of size \lstinline{NfemV} (the number 
of velocity degrees of freedom) and vectors this {\color{carrotorange} color} are 
vectors of size \lstinline{NfemP} (the number of pressure degrees of freedom).

In the context of the L2 projection, the second line of the algorithm 
is replaced by a new one.
we start instead from 
\[
\langle p^{k+1},q  \rangle = 
\langle p^{k},q  \rangle -\alpha_k \langle \vec\nabla \cdot \vec{\upnu}^{k} -h ,q \rangle 
\qquad \forall \quad q\in Q 
\]
where $\langle \cdot,\cdot  \rangle$ denotes the $L_2$ inner product on $Q$ (the pressure space):
\[
\langle p,q \rangle = \int_\Omega p(x)q(x) dx, \qquad \forall p,q \in Q.
\]
We then obtain:
\[
{\bm M}_p \cdot {\color{carrotorange} \vec{\cal P}_{k+1}} 
=
{\bm M}_p \cdot {\color{carrotorange} \vec{\cal P}_{k}}  
+\alpha_k  (\mathbb{G}^T\cdot {\color{teal} \vec{\cal V}_k}   - {\color{carrotorange}\vec{h}})
\]
where ${\bm M}_p$ is the pressure mass matrix.

In light of all this, I have written three solvers:
\begin{itemize}
\item Solver 15 is the standard Uzawa 1 ({\tt uzawa1\_solver.py}):
\begin{lstlisting}
for k in range (0,niter):
    solV=sps.linalg.spsolve(K_mat,f_rhs-G_mat.dot(solP))
    solPnew=solP+omega*(G_mat.T.dot(solV)-h_rhs)         
\end{lstlisting}

\item Solver 18 is Uzawa 1 with $L_2$ projection ({\tt uzawa1\_solver\_L2.py}):
\begin{lstlisting}
for k in range (0,niter):
    solV=sps.linalg.spsolve(K_mat,f_rhs-G_mat.dot(solP))     
    rhs=MP_mat.dot(solP)+omega*(G_mat.T.dot(solV)-h_rhs)      
    solPnew=sps.linalg.spsolve(MP_mat,rhs)   
\end{lstlisting}


\item Solver 19 is Uzawa 1 with $L_2$ projection in which 
the matrix multiplication with the pressure mass matrix is avoided ({\tt uzawa1\_solver\_L2b.py}):
\begin{lstlisting}
for k in range (0,niter):
    solV=sps.linalg.spsolve(K_mat,f_rhs-G_mat.dot(solP))   
    rhs=omega*(G_mat.T.dot(solV)-h_rhs)                     
    dsolP=sps.linalg.spsolve(MP_mat,rhs)   
    solPnew=solP+dsolP                                    
\end{lstlisting}

\end{itemize}

Compared to the standard Uzawa1 approach (\lstinline{solver=15}) these have an additional cost, 
since each iteration now involves a solve with the pressure mass matrix 
(and potentially a SpMV multiplication with this matrix too). 
However, the results are nothing short of spectacular:

\begin{center}
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/L2/uzawa1/solver_convergence.pdf}
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/L2/uzawa1/time.pdf}\\
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/L2/uzawa1/errorsV.pdf}
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/L2/uzawa1/errorsP.pdf}
\end{center}

Solvers 18 and 19 converge {\it much} faster than 15 (standard Uzawa1) and we find that 
they still yield correct discretisations errors (errors for solvers 18 and 19
are obviously identical).
The maximum number of iterations is arbitrarily set to 5000. We see that when resolution increases 
solver 15 fails to converge, even after 5000 iterations, and the measured errors reflect this.

When it comes to solve time, despite the extra solve with the 
pressure mass matrix solvers 18 and 19 are {\it much} faster than 15
and we find that 19 is a few percents faster than 18. 

Finally, we find that solver 15 gets better and better with increasing $\omega$
(up to a certain limit) while solvers 18 \& 19 will diverge for $\omega>3$.

Remark: in this example $\vec{h}=0$. One should verify that solvers 18,19 still work if not zero.

%---------------------------------------------------
\subsection*{Uzawa 2 algorithm}

Assume ${\color{carrotorange} \vec{\cal P}_0}$ known. 
Solve $\mathbb{K}\cdot {\color{teal} \vec{\cal V}_0} 
= {\color{teal} \vec{f}} - \mathbb{G}\cdot  {\color{carrotorange} \vec{\cal P}_0}$. 
For $k=0,1,2,...$, compute 
\begin{eqnarray}
{\color{carrotorange} \vec{\cal R}_k}
={\color{carrotorange} \vec{q}_k} 
&=& {\color{carrotorange} \vec{h}} - \mathbb{G}^T \cdot {\color{teal} \vec{\cal V}_k}  \nn\\
{\color{teal} \vec{p}_k} &=& {\G}\cdot {\color{carrotorange} \vec{q}_k} \nn\\
{\color{teal} \vec{H}_k} &=& {\K}^{-1}\cdot {\color{teal} \vec{p}_k } \nn\\
\alpha_k &=& \frac{  {\color{carrotorange} \vec{q}_k} \cdot {\color{carrotorange} \vec{q}_k}}
{ {\color{teal} \vec{p}_k} \cdot {\color{teal} \vec{H}_k }} \nn\\
{\color{carrotorange} \vec {\cal P}_k} &=& 
{\color{carrotorange} \vec {\cal P}_{k-1}} - \alpha_k  {\color{carrotorange} \vec q_k} \nn\\
{\color{teal} \vec {\cal V}_{k}} &=& {\color{teal} \vec {\cal V}_{k-1}} 
+ \alpha_k {\color{teal} \vec{H}_k}
\end{eqnarray}

The $L_2$ version has two differences:
\[
{\color{carrotorange} \vec{q}_k} = 
{\color{carrotorange} \vec{h}} - \mathbb{G}^T \cdot {\color{teal} \vec{\cal V}_k} 
\qquad
\Rightarrow
\qquad
\langle q^k,q \rangle  = \langle h+ \vec\nabla \cdot \vec\upnu^k,q \rangle \qquad \forall q
\]
\[
\alpha_k = \frac{  {\color{carrotorange} \vec{q}_k} \cdot {\color{carrotorange} \vec{q}_k}}
{ {\color{teal} \vec{p}_k} \cdot {\color{teal} \vec{H}_k }} 
\qquad
\Rightarrow
\qquad
\alpha_k = \frac{ \langle q^k,q^k  \rangle  }{ \langle \vec\nabla q , H_k  \rangle   }
\]




For this solver we need the pressure mass matrix ${\bm M}_p$ again, but also a new
matrix ${\bm H}$:

\[
\langle q,q \rangle 
= \vec{q}^T \cdot \int_\Omega \vec{\cal N}_p^T \vec{\cal N}_p dV \cdot \vec{q} = \vec{q}^T \cdot {\bm M}_p \cdot \vec{q}
\]
\[
\langle \nabla q,z\rangle = \int_\Omega  \nabla q \; z dV = 
\int (\partial_x q \; \partial_y q) \cdot 
\begin{pmatrix}
z_x \\ z_y
\end{pmatrix}
dV
\]
We have 
\[
(\partial_x q \; \partial_y q) =
(.... \vec{Q} ... ) \cdot
\begin{pmatrix}
\partial_x \bN_1^p & \partial_y \bN_1^ p\\
\partial_x \bN_2^p & \partial_y \bN_2^ p\\
\partial_x \bN_3^p & \partial_y \bN_3^ p\\
...  & ... \\
\partial_x \bN_N^p & \partial_y \bN_N^ p
\end{pmatrix}
\qquad
\text{and}
\qquad
\begin{pmatrix}
z_x \\ z_y
\end{pmatrix}
=
\begin{pmatrix}
\bN_1^v & 0 & \bN_2^v & 0 & ... \\
0 & \bN_1^v & 0 & \bN_2^v & ... \\
\end{pmatrix}
\cdot
\vec{\cal Z}
\]
so that 
\[
\langle \nabla q,z\rangle
= 
\vec{Q}^T \cdot 
\left[
\int_\Omega
\begin{pmatrix}
\partial_x \bN_1^p & \partial_y \bN_1^ p\\
\partial_x \bN_2^p & \partial_y \bN_2^ p\\
\partial_x \bN_3^p & \partial_y \bN_3^ p\\
...  & ... \\
\partial_x \bN_N^p & \partial_y \bN_N^ p
\end{pmatrix}
\cdot
\begin{pmatrix}
\bN_1^v & 0 & \bN_2^v & 0 & ... \\
0 & \bN_1^v & 0 & \bN_2^v & ... \\
\end{pmatrix}
dV \right]
\cdot
\vec{\cal Z}
\]
This translates as follows into code and is included in the 
matrix building for loop over all elements:

\begin{lstlisting}
H_mat=lil_matrix((Nfem_P,Nfem_V),dtype=np.float64)
for iel in range(0,nel):
    [...]
    for iq in range(0,nq_per_dim):
        for jq in range(0,nq_per_dim):
            [...]
            dNdx_P=jcbi[0,0]*dNdr_P+jcbi[0,1]*dNds_P
            dNdy_P=jcbi[1,0]*dNdr_P+jcbi[1,1]*dNds_P
            aa_mat[:,0]=dNdx_P[:]
            aa_mat[:,1]=dNdy_P[:]
            for i in range(0,m_V):
                bb_mat[0,2*i  ]=N_V[i] 
                bb_mat[1,2*i+1]=N_V[i] 
            H_el+=aa_mat@bb_mat*JxWq
\end{lstlisting}

Two solvers have then been written:
\begin{itemize}
\item Solver 16 is the standard Uzawa 2:
\begin{lstlisting}
for k in range (0,niter): 
    qk=h_rhs-G_mat.T.dot(solV)      
    pk=G_mat.dot(qk)               
    Hk=sps.linalg.spsolve(K_mat,pk) 
    alphak=qk.dot(qk)/pk.dot(Hk)   
    solPnew=solP-alphak*qk        
    solVnew=solV+alphak*Hk       
\end{lstlisting}

\item Solver 20 is Uzawa 2 with L2 projection: 
\begin{lstlisting}
for k in range (0,niter): 
    rhs=h_rhs-G_mat.T.dot(solV)
    qk=sps.linalg.spsolve(MP_mat,rhs,use_umfpack=False)    
    pk=G_mat.dot(qk)                           
    Hk=sps.linalg.spsolve(K_mat,pk)             
    numerator=qk.dot(MP_mat.dot(qk))             
    denominator=qk.dot(H_mat.dot(Hk))             
    alphak=numerator/denominator                   
    solPnew=solP-alphak*qk                          
    solVnew=solV+alphak*Hk                           
\end{lstlisting}

\end{itemize}

Again, the results are nothing short of spectacular:

\begin{center}
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/L2/uzawa2/solver_convergence_P.pdf}
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/L2/uzawa2/solver_convergence_V.pdf} \\
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/L2/uzawa2/solver_convergence_alpha.pdf}
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/L2/uzawa2/time.pdf}\\
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/L2/uzawa2/errorsV.pdf}
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/L2/uzawa2/errorsP.pdf}
\end{center}

Solver 20 is about a factor 10 faster than solver 16.
We also find that the value of $\alpha_k$ for solver 20 is resolution independent.

Remark: in this example $\vec{h}=0$. One should verify that solvers 18,19 still work if not zero.

%---------------------------------------------------
\subsection*{Uzawa 3 algorithm}

I recall here the structure of the solver (see Section~\ref{MMM-ss:schurpcg})
and highlight the lines which will need special treatment in the context of the $L_2$-projection
with a $\triangleleft$ (I have also removed the $\C$ matrix entries which are irrelevant here).

\begin{itemize}
\item compute ${\color{teal} \vec{\cal V}_0}=\K^{-1}\cdot ({\color{teal} \vec{f}}-\G \cdot {\color{carrotorange} \vec{\cal P}_0})$
\item $ {\color{carrotorange} \vec{r}_0} = \G^T\cdot {\color{teal} \vec{\cal V}_0} - {\color{carrotorange}\vec{h}}$  \hspace{1cm} $\triangleleft$
\item if $\vec{r}_0$ is sufficiently small, then return $(\vec{\cal V}_0,\vec{\cal P}_0)$ as the result
\item ${\color{carrotorange} \vec{p}_0}={\color{carrotorange} \vec{r}_0}$
\item $k=0$
\item repeat
\begin{enumerate}
\item compute ${\color{teal} \tilde{\vec{p}}_k} = \G \cdot {\color{carrotorange} \vec{p}_k}$
\item solve $\K\cdot {\color{teal} \vec{d}_k} = {\color{teal} \tilde{p}_k }$
\item compute $\alpha_k=( {\color{carrotorange} \vec{r}_k^T \cdot  \vec{r}_k})
/({\color{teal} \tilde{\vec{p}}_k^T \cdot \vec{d}_k })$ \hspace{1cm} $\triangleleft$
\item ${\color{carrotorange} \vec{\cal P}_{k+1}} = {\color{carrotorange} \vec{\cal P}_k}+\alpha_k {\color{carrotorange} \vec{p}_k}$
\item ${\color{teal} \vec{\cal V}_{k+1}} = {\color{teal} \vec{\cal V}_k} - \alpha_k {\color{teal} \vec{d}_k}$
\item ${\color{carrotorange} \vec{r}_{k+1}} = {\color{carrotorange}\vec{r}_k} 
-\alpha_k (\G^T \cdot {\color{teal} \vec{d}_k}) $ \hspace{1cm} $\triangleleft$
\item if $\vec{r}_{k+1}$ is sufficiently small ($||\vec{r}_{k+1}||_2/||\vec{r}_0||_2 <tol$), then exit loop
\item compute $\beta_k=({\color{carrotorange} \vec{r}_{k+1}^T \cdot \vec{r}_{k+1}})
/({\color{carrotorange} \vec{r}_k^T \cdot \vec{r}_k})$ \hspace{1cm} $\triangleleft$
\item ${\color{carrotorange} \vec{p}_{k+1}} = {\color{carrotorange} \vec{r}_{k+1} }+ \beta_k {\color{carrotorange}\vec{p}_k}$
\item $k=k+1$
\end{enumerate}
\item return $\vec{\cal V}_{k+1}$ and $\vec{\cal P}_{k+1}$ as result
\end{itemize}

The first residual $\vec{r}_0$ is then computed as follows:
\begin{lstlisting}
rvect_k=sps.linalg.spsolve(MP_mat,G_mat.T.dot(solV)-h_rhs) 
\end{lstlisting}
Then $\alpha_k$ is computed as follows:
\[
\alpha_k
= \frac{ \vec{r}_k^T \cdot  \vec{r}_k } { \tilde{\vec{p}}_k^T \cdot \vec{d}_k }
= \frac{ \vec{r}_k^T \cdot  \vec{r}_k } {(\G \cdot  \vec{p}_k)^T \cdot \vec{d}_k }
\quad
\Rightarrow
\quad
\alpha_k
= \frac{ \vec{r}_k^T \cdot {\bm M}_p \cdot \vec{r}_k } {\vec{p}_k^T \cdot {\bm H} \cdot \vec{d}_k }
\]
which translates into
\begin{lstlisting}
numerator=rvect_k.dot(MP_mat.dot(rvect_k))     
denominator=pvect_k.dot(H_mat.dot(dvect_k))   
alpha=numerator/denominator                  
\end{lstlisting}
The updated residual $\vec{r}_{k+1}$ follows. 
We can write step 6 
\[
\delta r\vec{r} = \vec{r}_{k+1} - \vec{r}_k = - \alpha_k (\G^T \cdot \vec{d}_k ) 
\]
which leads to
\begin{lstlisting}
dr=sps.linalg.spsolve(MP_mat,-alpha*G_mat.T.dot(dvect_k)) 
rvect_kp1=rvect_k+dr     
\end{lstlisting}
and finally $\beta_k$:
\[
\beta_k= \frac{ \vec{r}_{k+1}^T \cdot \vec{r}_{k+1} } { \vec{r}_k^T \cdot \vec{r}_k }
\quad
\Rightarrow
\quad
\beta_k= \frac{ \vec{r}_{k+1}^T \cdot {\bm M}_p \cdot \vec{r}_{k+1} }
{ \vec{r}_k^T \cdot {\bm M}_p \cdot \vec{r}_k }
\]
\begin{lstlisting}
numerator=rvect_kp1.dot(MP_mat.dot(rvect_kp1)) 
denominator=rvect_k.dot(MP_mat.dot(rvect_k))  
beta=numerator/denominator                   
\end{lstlisting}

In what follows solver 21 is identical to solver 4 (schur complement solver) but: 
different $\xi_{\cal V}=\|{\cal V}^{k+1}-{\cal V}\|_2$ and 
$\xi_{\cal P}=\|{\cal P}^{k+1}-{\cal P}^k\|_2$ calculation (convergence indicators) so as to 
compare with previous Uzawa 1 and 2 (and no preconditioner nor inner solver choice). 
Solver 22 is the Uzawa 3 solver (i.e solver 21) with $L_2$ projection.
I have also coded solver 23, which is solver 22, albeit the inner solve 
is carried out with a LU solver\footnote{\url{https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.splu.html}}: 
in practice the matrix $\K$ is then LU-decomposed once and the decomposition 
is then reused for each solve with $\K$. This results in a drastic
speedup as shown hereunder:

\begin{center}
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/L2/uzawa3/solver_convergence_P.pdf}
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/L2/uzawa3/solver_convergence_V.pdf} \\
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/L2/uzawa3/solver_convergence_alpha.pdf}
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/L2/uzawa3/time.pdf}\\
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/L2/uzawa3/errorsV.pdf}
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/L2/uzawa3/errorsP.pdf}
\end{center}

Again, the L2 projection improves on the standard Uzawa solver, and in 
this particular case the number of iterations is divided by 3, and the total solve 
time is also more than halved.
The direct solver approach (solver 1) is also included in the plots as reference. 
We see that solver 1 still remains substantially faster.
Of course the performance of the Uzawa solvers is tied to the required tolerance,
which is set to a low value of $10^{-7}$.

I have here also explored the influence of the tolerance parameter. 
It was previously always $10^{-7}$ and I here document its effect on 
accuracy and solve time. 
We find that $10^{-5}$ (or even $10^{-4}$) seems to be sufficient (for this one isoviscous manufactured solution
and this range of resolutions). Since less iterations are needed it makes the iterative 
solvers faster, especially 21 since it 'spares' dozens of iterations. 
For solver 22 we find:

\begin{center}
\begin{tabular}{|c|cc|cc|cc|}
\hline
& tol=$10^{-4}$ & & tol=$10^{-5}$ & & tol=$10^{-7}$ & \\
nel   & time (s) & \# its &  time (s) & \# its &  time (s) & \# its  \\ 
\hline
1024  & 1.748    &  7  &2.046  &   8  & 2.073    & 10 \\
1600  & 4.159    &  8  &4.557  &   8  & 4.716    & 10 \\
2304  & 8.103    &  8  &8.358  &   9  &  9.374   & 11 \\
4096  & 18.581   &  8  &20.229 &   9  &  22.501  & 11 \\
6400  & 34.032   &  8  &36.868 &   9  &  41.397  & 11 \\
9216  & 73.158   &  9  &72.908 &   9  &  84.936  & 11 \\
16384 & 192.415  &  9  &205.503&   10 &  226.167 & 11 \\
25600 &          &     &       &      &  436.069 & 12 \\
36864 &          &     &       &      &  786.035 & 12 \\
\hline
\end{tabular}
\end{center}


One can also look at the time per solver iteration
\begin{center}
\includegraphics[width=10cm]{python_codes/fieldstone_147/RESULTS/L2/uzawa3/time_per_iteration.pdf}
\end{center}
We find that the additional solves and matrix-vector multiplications for the L2 variant (solver 22)
do not make each iteration substantially longer.

Since solver 23 relies on LU and therefore needs to store the $L$ and $U$
matrices, I expect it requires more memory than solver 22 - not proven yet.

\newpage
%====================================================
\section*{Donea \& Huerta mms With preconditioner}

Following this site\footnote{\url{https://caam37830.github.io/book/02_linear_algebra/sparse_linalg.html}}
we build an ILU preconditioner as follows:
\begin{lstlisting}
ILUfact = sla.spilu(sparse_matrix)
M = sla.LinearOperator(
    shape = sparse_matrix.shape,
    matvec = lambda b: ILUfact.solve(b))
\end{lstlisting}
However, I find that the preconditioner does not work with gmres, minres, cg, ...

This means that I need to build my own preconditioner. 
For the SC-CG 

We have designed X preconditioners:
\begin{itemize}
\item {\tt precond\_type=0} It is the unit matrix (so it does nothing). 
%\item {\tt precond\_type=1} This is a very simple one as it is
%diagonal and built element by element:
%\[
%M_{e,e} = \frac{h_x h_y}{\eta_e} 
%\]
%where $e$ is an element and $\eta_e$ the viscosity evaluated in its center. Note that 
%averages based on quadrature point values could also be considered (or any other kind of projection).

\item {\tt precond\_type=2}
\[
{\bm M} = \G^T (diag [\K]  )^{-1} \G 
\]
\item {\tt precond\_type=3} 
\[
{\bm M} = diag \left[ \G^T (diag [\K]  )^{-1} \G \right]
\]
\item {\tt precond\_type=4} Same as 2, but instead of using the 
diagonal of $ \G^T (diag [\K]  )^{-1} \G$ we lump the matrix instead.

\end{itemize}

{\color{red} \Large unfinished}
