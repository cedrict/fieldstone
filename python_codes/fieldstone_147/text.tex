\noindent
\includegraphics[height=1.25cm]{images/pictograms/benchmark}
\includegraphics[height=1.25cm]{images/pictograms/under_construction}
\includegraphics[height=1.25cm]{images/pictograms/FEM}
\includegraphics[height=1.25cm]{images/pictograms/paraview}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{flushright} {\tiny {\color{gray} python\_codes/fieldstone\_147/text.tex}} \end{flushright}

\lstinputlisting[language=bash,basicstyle=\small]{python_codes/fieldstone_147/keywords.key}

\par\noindent\rule{\textwidth}{0.4pt}

\begin{center}
\inpython
{\small Code: \url{https://github.com/cedrict/fieldstone/tree/master/python_codes/fieldstone_147}}
\end{center}

\par\noindent\rule{\textwidth}{0.4pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The idea behind this \stone is the comparison of various approaches to solve the 
Stokes system. We here consider simple setups in a 2D rectangular domain. 
The code relies on the \QtwoQone element pair and is based on \stone~\ref{f18}.

In almost all previous stones I have used {\python spsolve} to solve the linear system
but looking online we find that there are many more methods 
available\footnote{\url{https://docs.scipy.org/doc/scipy/reference/sparse.linalg.html}}.
Of all available I highlight the following ones:

\begin{itemize}
\item Direct methods for linear equation systems
\begin{itemize}
\item {\python spsolve}:  
It looks like it is the SuperLU solver\footnote{\url{https://caam37830.github.io/book/02_linear_algebra/sparse_linalg.html}}. 
\item {\python spsolve+UMFPACK}: ({\python solver=11}) 
\end{itemize}
\item Iterative methods for linear equation systems:
\begin{itemize}
\item {\python bicg}(A, b[, x0, tol, maxiter, M, callback, atol]) ({\python solver=9})
\item {\python bicgstab}(A, b[, x0, tol, maxiter, M, ...]) ({\python solver=10})
\item {\python cg}(A, b[, x0, tol, maxiter, M, callback, atol])
\item {\python cgs}(A, b[, x0, tol, maxiter, M, callback, atol])  ({\python solver=12})
\item {\python gmres}(A, b[, x0, tol, restart, maxiter, M, ...]) ({\python solver=2})
\item {\python lgmres}(A, b[, x0, tol, maxiter, M, ...]) ({\python solver=3})
\item {\python minres}(A, b[, x0, shift, tol, maxiter, M, ...]) ({\python solver=5})
\item {\python qmr}(A, b[, x0, tol, maxiter, M1, M2, ...]) ({\python solver=6})
\item {\python gcrotmk}(A, b[, x0, tol, maxiter, M, ...]) ({\python solver=8})
\item {\python tfqmr}(A, b[, x0, tol, maxiter, M, callback, ...]) ({\python solver=7})
\end{itemize}
\end{itemize}

If we build the system as a single (sparse) two-dimensional array 
we can right away rule out {\python cg} which requires 
a SPD matrix\footnote{\url{https://en.wikipedia.org/wiki/Conjugate_gradient_method}}. 
We will then try all the other ones.

Additionally we can build the $\K$ and $\G$ blocks and use conjugate gradient method on 
the Schur complement equation ('SC-CG'), as explained in Section~\ref{MMM-ss:schurpcg}.\\
This solver is implemented in {\tt schur\_complement\_cg\_solver.py}.
It requires an inner solver called once per iteration on the $\K$ block.
We then have the choice between using a direct solver ({\python solver=4})
or using a conjugate gradient
approach since $\K$ is SPD ({\python solve=13}). 
By using CG we pretty much ensure that memory 
requirements remain very low.
Another variant consists in acknowledging that inside the iterations the 
block $\K$ does not change, so that we can compute its LU decomposition
and use this decomposition every time that we need to solve a linear
system with $\K$. This is {\python solver=14}. 
This option means that the LU factors are stored in the memory longer than in the standard
case but with equal memory use.

All these methods should also be used with a preconditioner, which we will 
investigate later on.

Iterative solver require a tolerance value and it is set to 1e-7 unless mentioned
otherwise.

\newpage

\begin{enumerate}
%-----------------01
\item the first solver is the usual one I use in all the stones
\begin{lstlisting}
   sol=sps.linalg.spsolve(sparse_matrix,rhs,use_umfpack=False)
\end{lstlisting}
%-----------------02
\item the second solver is the gmres from linalg 
\begin{lstlisting}
   sol,info = scipy.sparse.linalg.gmres(sparse_matrix, rhs, restart=2000,tol=tolerance,M=Mprec)
\end{lstlisting}
%-----------------03
\item the third one is the lgmres from linalg
\begin{lstlisting}
   sol = scipy.sparse.linalg.lgmres(sparse_matrix, rhs,atol=1e-16,tol=tolerance)[0]
\end{lstlisting}
%-----------------04
\item the fourth one is my Schur complement CG solver using direct solver for inner solve
\begin{lstlisting}
   solV,p,niter=schur_complement_cg_solver(K_mat,G_mat,M_mat,f_rhs,h_rhs,\
                                          NfemV,NfemP,niter_max,tolerance,use_precond,'direct')
\end{lstlisting}
%-----------------05
\item the fifth one is minres from linalg
\begin{lstlisting}
   sol = scipy.sparse.linalg.minres(sparse_matrix, rhs, tol=1e-10)[0]
\end{lstlisting}
%-----------------06
\item the sixth one is the qmr from linalg
\begin{lstlisting}
   sol = scipy.sparse.linalg.qmr(sparse_matrix, rhs, tol=tolerance)[0]
\end{lstlisting}
%-----------------07
\item the seventh one is the tfqmr from linalg
\begin{lstlisting}
   sol = scipy.sparse.linalg.tfqmr(sparse_matrix, rhs, tol=1e-10)[0]
\end{lstlisting}
%-----------------08
\item the eighth one is the gcrotmk from linalg
\begin{lstlisting}
   sol = scipy.sparse.linalg.gcrotmk(sparse_matrix, rhs,atol=1e-16, tol=1e-10)[0]
\end{lstlisting}
%-----------------09
\item the ninth one is the bicg from linalg
\begin{lstlisting}
   sol = scipy.sparse.linalg.bicg(sparse_matrix, rhs, tol=tolerance)[0]
\end{lstlisting}
%-----------------10
\item the tenth one is bicgstab from linalg
\begin{lstlisting}
   sol = scipy.sparse.linalg.bicgstab(sparse_matrix, rhs, tol=tolerance)[0]
\end{lstlisting}
%-----------------11
\item the eleventh one is spsolve+umfpack 
\begin{lstlisting}
   sol=sps.linalg.spsolve(sparse_matrix,rhs,use_umfpack=True)
\end{lstlisting}
%-----------------12
\item the twelfth one is cgs from linalg
\begin{lstlisting}
   sol = scipy.sparse.linalg.cgs(sparse_matrix, rhs, tol=tolerance)[0]
\end{lstlisting}
%-----------------13
\item the thirteenth one is my Schur complement CG solver using cg for inner solve
\begin{lstlisting}
   solV,p,niter=schur_complement_cg_solver(K_mat,G_mat,M_mat,f_rhs,h_rhs,\
                                           NfemV,NfemP,niter_max,tolerance,use_precond,'cg')
\end{lstlisting}
%-----------------14
\item the fourteenth one is my Schur complement CG solver using splu for inner solve
\begin{lstlisting}
   solV,p,niter=schur_complement_cg_solver(K_mat,G_mat,M_mat,f_rhs,h_rhs,\
                                           NfemV,NfemP,niter_max,tolerance,use_precond,'splu')
\end{lstlisting}
%-----------------15
\item uzawa 1: Section 5.1 \cite{braess}. Parameterised by $\omega$
\begin{lstlisting}
   solV,p,niter=uzawa1_solver(K_mat,G_mat,M_mat,f_rhs,h_rhs,\
                       NfemV,NfemP,niter_max,tolerance,use_precond,'direct',omega)
\end{lstlisting}

%-----------------16
\item uzawa 2: Section 5.2 \cite{braess}. 
\begin{lstlisting}
   solV,p,niter=uzawa2_solver(K_mat,G_mat,M_mat,f_rhs,h_rhs,\
                       NfemV,NfemP,niter_max,tolerance,use_precond,'direct')
\end{lstlisting}

%-----------------17
\item projection solver 
\begin{lstlisting}
solV,p,niter=projection_solver(K_mat,G_mat,L_mat,f_rhs,h_rhs,NfemV,NfemP,niter_max,tolerance)
\end{lstlisting}

%-----------------18
\item Uzawa 1 + L2 projection
\begin{lstlisting}
solV,p,niter=uzawa1_solver_L2(K_mat,G_mat,MP_mat,f_rhs,h_rhs,NfemP,niter_max,tolerance,omega)
\end{lstlisting}

%-----------------19
\item Uzawa 1b + L2 projection
\begin{lstlisting}
solV,p,niter=uzawa1_solver_L2b(K_mat,G_mat,MP_mat,f_rhs,h_rhs,NfemP,niter_max,tolerance,omega)
\end{lstlisting}

%-----------------20
\item Uzawa 2 + L2 projection
\begin{lstlisting}
solV,p,niter=uzawa2_solver_L2(K_mat,G_mat,MP_mat,H_mat,f_rhs,h_rhs,NfemP,niter_max,tolerance)
\end{lstlisting}

%-----------------21
\item Uzawa 3 (CG on Schur) projection
\begin{lstlisting}
solV,p,niter=uzawa3_solver(K_mat,G_mat,f_rhs,h_rhs,NfemP,niter_max,tolerance)
\end{lstlisting}

%-----------------22
\item Uzawa 3 (CG on Schur) + L2 projection
\begin{lstlisting}
solV,p,niter=uzawa3_solver_L2(K_mat,G_mat,MP_mat,H_mat,f_rhs,h_rhs,NfemP,niter_max,tolerance,'direct')
\end{lstlisting}

%-----------------23
\item Uzawa 3 (CG on Schur), LU inner, L2 projection
\begin{lstlisting}
solV,p,niter=uzawa3_solver_L2(K_mat,G_mat,MP_mat,H_mat,f_rhs,h_rhs,NfemP,niter_max,tolerance,'splu')
\end{lstlisting}


\end{enumerate}


\newpage
%====================================================
\section*{Donea \& Huerta mms Without preconditioner}

\begin{center}
\includegraphics[width=12cm]{python_codes/fieldstone_147/RESULTS/errors.pdf}
\end{center}

\begin{center}
\includegraphics[width=12cm]{python_codes/fieldstone_147/RESULTS/solve.pdf} \\
\includegraphics[width=12cm]{python_codes/fieldstone_147/RESULTS/solve2.pdf}\\
{\captionfont Time to solve linear system. Only results for usable solvers are shown. 
Left: time as a function of the number of elements; Right: 
time as a function of the total number of dofs.}
\end{center}

\begin{center}
\includegraphics[width=12cm]{python_codes/fieldstone_147/RESULTS/convergence_schur_cpl.pdf}\\
{\captionfont Convergence of the SC-CG solver as function of resolution.}
\end{center}

On the following figure I plot the velocity and pressure errors for SC-CG+ilu solver 
for different values of the relative tolerance. We find that the solve time is not 
much influenced by the tolerance, but unsurprisingly the accuracy of the solution is: 
\begin{center}
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/tol_study_solver14/errors.pdf}
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/tol_study_solver14/solve.pdf}\\
{\captionfont Left: Influence of the relative tolerance on the accuracy of the solution obtained 
with the SC-CG+ilu ({\python solver=14}). Right: solve time in seconds.}
\end{center}
It appears that a relative tolerance of $10^{-6}$ is enough, but I set it to $10^{-7}$ default
to be on the safe side (when looking closley at high resolution it looks like the 
blue curve of $10^{-6}$  seems to separate from the orange one from $10^{-7}$).

I have also run \aspect on the same problem (with the same type of elements) and on only 
1 core (while the spsolve inner solve tends to use all available cores of my laptop). 
Since \aspect reports the assembly time, the preconditioner time and the solve
time separately I add them together.
We find that the solver outperforms mine (obviously!) and more importantly that 
is scales linearly with the number of elements. 
The highest resolutions so far are $300\times 300$ elements and the memory requirements 
were about half of my laptop RAM, so about 16Gb, and $400\times 400$ which requires
about 20+ Gb.


We can also look at the $L_2$ error on the velocity divergence:

\begin{center}
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/error_divv.pdf}
\end{center}

\newpage
%......................................
\subsubsection*{About the Uzawa solvers}

This subsection was written and used to review a paper.

U1 is uzawa of Braess, section 5.1: simple to implement but a parameter is needed. 
U2 is uzawa of Braess, section 5.2
U3 is conjugate gradient on schur complement, section 5.3.

The three algorithms U1, U2, U3 are reproduced from \cite{braess} hereunder from left to right for convenience:
\begin{center}
\fbox{\includegraphics[width=5.6cm]{python_codes/fieldstone_147/images/braess_U1}}
\fbox{\includegraphics[width=5.6cm]{python_codes/fieldstone_147/images/braess_U2}}
\fbox{\includegraphics[width=5.6cm]{python_codes/fieldstone_147/images/braess_U3}}\\
Taken from \textcite{braess}.
\end{center}

The following figure shows the convergence of this solver for different values of this parameter.
20000 makes the solver explode.	

\begin{center}
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/uzawa123/convergence_24x24.pdf}
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/uzawa123/convergence_48x48.pdf}\\
{\captionfont Convergence on 24x24 mesh (left) and 48x48 mesh (right) 
for Donea \& Huerta manufactured solution.}
\end{center}

We see that the $\alpha$ parameter should be a function of resolution, and that 
the uzawa2 converges in the same number of iterations because it automatically 
computes the step.


Actually when focusing now on U2 and U3 only and running the experiment at increasing resolutions I find ({\it as expected for stable element pairs}) that the number of iterations is independent of the resolution:
\begin{center}
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/uzawa123/convergence_uzawa23}
\end{center}



This shows that\\
1) U2 is a much better/safe algorithm than U1 while being equally simple to implement, thereby proving that U1
 should be discarded altogether; \\
2) U3 is the obvious starting point if one wishes to offer an improvement on Uzawa-type solvers as it is by far the best of the three and is marginally more complex to implement than U1 or U2.






\newpage
%-------------------------
\subsection*{Conclusions}

\begin{itemize}
\item direct method spsolve about 10x faster than all others (except solver 14!)
\item UMFPACK option in spsolve does not yield substantial differences
\item my own Schur complement CG solver rivals other LINALG solvers (solver 4, 13) ! 
\item MINRES, even with very low tolerance 1e-10, ultimately fails
\item LGMRES seems to work fine, very similar to GMRES but does not always converge.
\item TFQMR, BICGSTAB, CGS are out: they fail to converge and/or return 
very wrong results
\item GCROTMK usable but slowest, also fails at high resolution 
\item number of iterations of SC-CG is independent of resolution, as expected
from a stable element pair (for isoviscous flow)
\item solve time for iterative method rather comparable, no clear best
\item SC-CG with direct or cg inner solve are comparable in terms of 
solve time. There is still something wrong about my having to 
set tol of cg extremely low, and even then it ends up crashing at nel=52 ...
\item my own SC-CG+ilu is faster than spsolve!
\end{itemize}

In the end, I keep SPSOLVE, SC-CG, GMRES, QMR, BICG.

\newpage
%====================================================
\section*{Donea \& Huerta mms With preconditioner}

Following this site\footnote{\url{https://caam37830.github.io/book/02_linear_algebra/sparse_linalg.html}}
we build an ILU preconditioner as follows:
\begin{lstlisting}
ILUfact = sla.spilu(sparse_matrix)
M = sla.LinearOperator(
    shape = sparse_matrix.shape,
    matvec = lambda b: ILUfact.solve(b))
\end{lstlisting}
However, I find that the preconditioner does not work with gmres, minres, cg, ...

This means that I need to build my own preconditioner. 
For the SC-CG 

We have designed X preconditioners:
\begin{itemize}
\item {\tt precond\_type=0} It is the unit matrix (so it does nothing). 
%\item {\tt precond\_type=1} This is a very simple one as it is
%diagonal and built element by element:
%\[
%M_{e,e} = \frac{h_x h_y}{\eta_e} 
%\]
%where $e$ is an element and $\eta_e$ the viscosity evaluated in its center. Note that 
%averages based on quadrature point values could also be considered (or any other kind of projection).

\item {\tt precond\_type=2}
\[
{\bm M} = \G^T (diag [\K]  )^{-1} \G 
\]
\item {\tt precond\_type=3} 
\[
{\bm M} = diag \left[ \G^T (diag [\K]  )^{-1} \G \right]
\]
\item {\tt precond\_type=4} Same as 2, but instead of using the 
diagonal of $ \G^T (diag [\K]  )^{-1} \G$ we lump the matrix instead.

\end{itemize}

{\color{red} \Large unfinished}

\newpage
%==============================================================
\section*{L2 projection stuff - Donea \& Huerta}

In what follows we will need to replace $\vec{p}^T\cdot\vec{p}$ 
by 
\[
\langle p,q \rangle = \int_\Omega {p} \cdot {q} dV 
= \vec{p}^T \cdot {\bm M}_{p} \cdot \vec{q}
\]
where ${\bm M}_{p}$ is the pressure mass matrix. 
Building and re-using the pressure mass matrix is simpler than 
computing the integral everytime, but it may not be 
the fastest approach.

Likewise we will need to replace $( \G \cdot \vec{x})^T\cdot \vec{y}$
by
\[
\langle \nabla {x},{y} \rangle = \int_\Omega \nabla \vec{x} \; {y} dV = \vec{x}^T \cdot {\bm H} \cdot \vec{y}
\]
where ${\bm H}$ is defined as $\int \vec\nabla \vec{\bN}_P  \cdot \vec{\bN}_V dV$ (not exactly, see below). 

Note that in the case of this benchmark the velocity boundary conditions
are no slip on all sides, so that in practive $\vec{h}=0$.

%---------------------------------------------------
\subsection*{Uzawa 1 algorithm}


Assume $\vec{\cal P}_0$ known
\begin{eqnarray}
\text{solve} \qquad \mathbb{K} \cdot {\color{teal} \vec{\cal V}_k} 
&=& {\color{teal} \vec{f}} - \mathbb{G}\cdot {\color{carrotorange} \vec{\cal P}_{k}}  \nn\\
{\color{carrotorange} \vec{\cal P}_{k+1}} &=& 
{\color{carrotorange} \vec{\cal P}_{k}}  
+ \alpha_k (\mathbb{G}^T\cdot {\color{teal} \vec{\cal V}_k}   - {\color{carrotorange}\vec{h}})
\quad
\quad
\quad
\quad
k=0,1,2, ... 
\end{eqnarray}

Vectors this {\color{teal} color} are vectors of size \lstinline{NfemV} (the number 
of velocity degrees of freedom) and vectors this {\color{carrotorange} color} are 
vectors of size \lstinline{NfemP} (the number of pressure degrees of freedom).

In the context of the L2 projection, the second line of the algorithm 
is replaced by a new one.
we start instead from 
\[
\langle p^{k+1},q  \rangle = 
\langle p^{k},q  \rangle -\alpha_k \langle \vec\nabla \cdot \vec{\upnu}^{k} -h ,q \rangle 
\qquad \forall \quad q\in Q 
\]
where $\langle \cdot,\cdot  \rangle$ denotes the $L_2$ inner product on $Q$ (the pressure space):
\[
\langle p,q \rangle = \int_\Omega p(x)q(x) dx, \qquad \forall p,q \in Q.
\]
We then obtain:
\[
{\bm M}_p \cdot {\color{carrotorange} \vec{\cal P}_{k+1}} 
=
{\bm M}_p \cdot {\color{carrotorange} \vec{\cal P}_{k}}  
+\alpha_k  (\mathbb{G}^T\cdot {\color{teal} \vec{\cal V}_k}   - {\color{carrotorange}\vec{h}})
\]
where ${\bm M}_p$ is the pressure mass matrix.

In light of all this, I have written three solvers:
\begin{itemize}
\item Solver 15 is the standard Uzawa 1:
\begin{lstlisting}
for k in range (0,niter):
    solV=sps.linalg.spsolve(K_mat,f_rhs-G_mat.dot(solP))
    solPnew=solP+omega*(G_mat.T.dot(solV)-h_rhs)         
\end{lstlisting}

\item Solver 18 is Uzawa 1 with $L_2$ projection:
\begin{lstlisting}
for k in range (0,niter):
    solV=sps.linalg.spsolve(K_mat,f_rhs-G_mat.dot(solP))     
    rhs=MP_mat.dot(solP)+omega*(G_mat.T.dot(solV)-h_rhs)      
    solPnew=sps.linalg.spsolve(MP_mat,rhs)   
\end{lstlisting}


\item Solver 19 is Uzawa 1 with $L_2$ projection in which 
the matrix multiplication with the pressure mass matrix is avoided:
\begin{lstlisting}
for k in range (0,niter):
    solV=sps.linalg.spsolve(K_mat,f_rhs-G_mat.dot(solP))   
    rhs=omega*(G_mat.T.dot(solV)-h_rhs)                     
    dsolP=sps.linalg.spsolve(MP_mat,rhs)   
    solPnew=solP+dsolP                                    
\end{lstlisting}

\end{itemize}

Compared to the standard Uzawa1 approach (solver 15) these have a cost, 
since each iteration now involves a solve with the pressure mass matrix 
(and potentially a SpMV multiplication with this matrix too). 

However, the results are nothing short of spectacular:

\begin{center}
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/L2/uzawa1/solver_convergence.pdf}
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/L2/uzawa1/time.pdf}\\
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/L2/uzawa1/errorsV.pdf}
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/L2/uzawa1/errorsP.pdf}
\end{center}

Solvers 18 and 19 converge {\it much} faster than 15 (standard Uzawa1) and we find that 
they still yield correct discretisations errors (errors for solvers 18 and 19
are obviously identical).
The maximum number of iterations is arbitrarily set to 5000. We see that when resolution increases 
solver 15 fails to converge, even after 5000 iterations, and the measured errors reflect this.

When it comes to solve time, despite the extra solve with the 
pressure mass matrix solvers 18 and 19 are {\it much} faster than 15
and we find that 19 is a few percents faster than 18. 

Finally, we find that solver 15 gets better and better with increasing $\omega$
(up to a certain limit) while solvers 18 \& 19 will diverge for $\omega>3$.

Remark: in this example $\vec{h}=0$. One should verify that solvers 18,19 still work if not zero.

%---------------------------------------------------
\subsection*{Uzawa 2 algorithm}

Assume ${\color{carrotorange} \vec{\cal P}_0}$ known. 
Solve $\mathbb{K}\cdot {\color{teal} \vec{\cal V}_0} 
= {\color{teal} \vec{f}} - \mathbb{G}\cdot  {\color{carrotorange} \vec{\cal P}_0}$. 
For $k=0,1,2,...$, compute 
\begin{eqnarray}
{\color{carrotorange} \vec{\cal R}_k}
={\color{carrotorange} \vec{q}_k} 
&=& {\color{carrotorange} \vec{h}} - \mathbb{G}^T \cdot {\color{teal} \vec{\cal V}_k}  \nn\\
{\color{teal} \vec{p}_k} &=& {\G}\cdot {\color{carrotorange} \vec{q}_k} \nn\\
{\color{teal} \vec{H}_k} &=& {\K}^{-1}\cdot {\color{teal} \vec{p}_k } \nn\\
\alpha_k &=& \frac{  {\color{carrotorange} \vec{q}_k} \cdot {\color{carrotorange} \vec{q}_k}}
{ {\color{teal} \vec{p}_k} \cdot {\color{teal} \vec{H}_k }} \nn\\
{\color{carrotorange} \vec {\cal P}_k} &=& 
{\color{carrotorange} \vec {\cal P}_{k-1}} - \alpha_k  {\color{carrotorange} \vec q_k} \nn\\
{\color{teal} \vec {\cal V}_{k}} &=& {\color{teal} \vec {\cal V}_{k-1}} 
+ \alpha_k {\color{teal} \vec{H}_k}
\end{eqnarray}

The $L_2$ version has two differences:
\[
{\color{carrotorange} \vec{q}_k} = 
{\color{carrotorange} \vec{h}} - \mathbb{G}^T \cdot {\color{teal} \vec{\cal V}_k} 
\qquad
\Rightarrow
\qquad
\langle q^k,q \rangle  = \langle h+ \vec\nabla \cdot \vec\upnu^k,q \rangle \qquad \forall q
\]
\[
\alpha_k = \frac{  {\color{carrotorange} \vec{q}_k} \cdot {\color{carrotorange} \vec{q}_k}}
{ {\color{teal} \vec{p}_k} \cdot {\color{teal} \vec{H}_k }} 
\qquad
\Rightarrow
\qquad
\alpha_k = \frac{ \langle q^k,q^k  \rangle  }{ \langle \vec\nabla q , H_k  \rangle   }
\]




For this solver we need the pressure mass matrix ${\bm M}_p$ again, but also a new
matrix ${\bm H}$:

\[
\langle q,q \rangle 
= \vec{q}^T \cdot \int_\Omega \vec{\cal N}_p^T \vec{\cal N}_p dV \cdot \vec{q} = \vec{q}^T \cdot {\bm M}_p \cdot \vec{q}
\]
\[
\langle \nabla q,z\rangle = \int_\Omega  \nabla q \; z dV = 
\int (\partial_x q \; \partial_y q) \cdot 
\begin{pmatrix}
z_x \\ z_y
\end{pmatrix}
dV
\]
We have 
\[
(\partial_x q \; \partial_y q) =
(.... \vec{Q} ... ) \cdot
\begin{pmatrix}
\partial_x \bN_1^p & \partial_y \bN_1^ p\\
\partial_x \bN_2^p & \partial_y \bN_2^ p\\
\partial_x \bN_3^p & \partial_y \bN_3^ p\\
...  & ... \\
\partial_x \bN_N^p & \partial_y \bN_N^ p
\end{pmatrix}
\qquad
\text{and}
\qquad
\begin{pmatrix}
z_x \\ z_y
\end{pmatrix}
=
\begin{pmatrix}
\bN_1^v & 0 & \bN_2^v & 0 & ... \\
0 & \bN_1^v & 0 & \bN_2^v & ... \\
\end{pmatrix}
\cdot
\vec{\cal Z}
\]
so that 
\[
\langle \nabla q,z\rangle
= 
\vec{Q}^T \cdot 
\left[
\int_\Omega
\begin{pmatrix}
\partial_x \bN_1^p & \partial_y \bN_1^ p\\
\partial_x \bN_2^p & \partial_y \bN_2^ p\\
\partial_x \bN_3^p & \partial_y \bN_3^ p\\
...  & ... \\
\partial_x \bN_N^p & \partial_y \bN_N^ p
\end{pmatrix}
\cdot
\begin{pmatrix}
\bN_1^v & 0 & \bN_2^v & 0 & ... \\
0 & \bN_1^v & 0 & \bN_2^v & ... \\
\end{pmatrix}
dV \right]
\cdot
\vec{\cal Z}
\]
This translates as follows into code and is included in the 
matrix building for loop over all elements:

\begin{lstlisting}
H_mat   = lil_matrix((NfemP,NfemV),dtype=np.float64) # matrix H 
for iel in range(0,nel):
    [...]
    for iq in range(0,nqperdim):
        for jq in range(0,nqperdim):
            [...]
            dNNNPdx[:]=jcbi[0,0]*dNNNPdr[:]+jcbi[0,1]*dNNNPds[:]
            dNNNPdy[:]=jcbi[1,0]*dNNNPdr[:]+jcbi[1,1]*dNNNPds[:]
            for i in range(0,mP):
                aa_mat[i,0]=dNNNPdx[i]
                aa_mat[i,1]=dNNNPdy[i]
            for i in range(0,mV):
                bb_mat[0,2*i  ]=NNNV[i] 
                bb_mat[1,2*i+1]=NNNV[i] 
            H_el+=aa_mat@bb_mat*weightq*jcob
\end{lstlisting}

Two solvers have then been written:
\begin{itemize}
\item Solver 16 is the standard Uzawa 2:
\begin{lstlisting}
for k in range (0,niter): 
    qk=h_rhs-G_mat.T.dot(solV)      
    pk=G_mat.dot(qk)               
    Hk=sps.linalg.spsolve(K_mat,pk) 
    alphak=qk.dot(qk)/pk.dot(Hk)   
    solPnew=solP-alphak*qk        
    solVnew=solV+alphak*Hk       
\end{lstlisting}

\item Solver 20 is Uzawa 2 with L2 projection: 
\begin{lstlisting}
for k in range (0,niter): 
    rhs=h_rhs-G_mat.T.dot(solV)
    qk=sps.linalg.spsolve(MP_mat,rhs,use_umfpack=False)    
    pk=G_mat.dot(qk)                           
    Hk=sps.linalg.spsolve(K_mat,pk)             
    numerator=qk.dot(MP_mat.dot(qk))             
    denominator=qk.dot(H_mat.dot(Hk))             
    alphak=numerator/denominator                   
    solPnew=solP-alphak*qk                          
    solVnew=solV+alphak*Hk                           
\end{lstlisting}

\end{itemize}





Again, the results are nothing short of spectacular:

\begin{center}
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/L2/uzawa2/solver_convergence_P.pdf}
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/L2/uzawa2/solver_convergence_V.pdf} \\
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/L2/uzawa2/solver_convergence_alpha.pdf}
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/L2/uzawa2/time.pdf}\\
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/L2/uzawa2/errorsV.pdf}
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/L2/uzawa2/errorsP.pdf}
\end{center}

Solver 20 is about a factor 10 faster than solver 16.
We also find that the value of $\alpha_k$ for solver 20 is resolution independent.

Remark: in this example $\vec{h}=0$. One should verify that solvers 18,19 still work if not zero.

%---------------------------------------------------
\subsection*{Uzawa 3 algorithm}

I recall here the structure of the solver (see Section~\ref{MMM-ss:schurpcg})
and highlight the lines which will need special treatment in the context of the $L_2$-projection
with a $\triangleleft$ (I have also removed the $\C$ matrix entries which are irrelevant here).

\begin{itemize}
\item compute ${\color{teal} \vec{\cal V}_0}=\K^{-1}\cdot ({\color{teal} \vec{f}}-\G \cdot {\color{carrotorange} \vec{\cal P}_0})$
\item $ {\color{carrotorange} \vec{r}_0} = \G^T\cdot {\color{teal} \vec{\cal V}_0} - {\color{carrotorange}\vec{h}}$  \hspace{1cm} $\triangleleft$
\item if $\vec{r}_0$ is sufficiently small, then return $(\vec{\cal V}_0,\vec{\cal P}_0)$ as the result
\item ${\color{carrotorange} \vec{p}_0}={\color{carrotorange} \vec{r}_0}$
\item $k=0$
\item repeat
\begin{enumerate}
\item compute ${\color{teal} \tilde{\vec{p}}_k} = \G \cdot {\color{carrotorange} \vec{p}_k}$
\item solve $\K\cdot {\color{teal} \vec{d}_k} = {\color{teal} \tilde{p}_k }$
\item compute $\alpha_k=( {\color{carrotorange} \vec{r}_k^T \cdot  \vec{r}_k})
/({\color{teal} \tilde{\vec{p}}_k^T \cdot \vec{d}_k })$ \hspace{1cm} $\triangleleft$
\item ${\color{carrotorange} \vec{\cal P}_{k+1}} = {\color{carrotorange} \vec{\cal P}_k}+\alpha_k {\color{carrotorange} \vec{p}_k}$
\item ${\color{teal} \vec{\cal V}_{k+1}} = {\color{teal} \vec{\cal V}_k} - \alpha_k {\color{teal} \vec{d}_k}$
\item ${\color{carrotorange} \vec{r}_{k+1}} = {\color{carrotorange}\vec{r}_k} 
-\alpha_k (\G^T \cdot {\color{teal} \vec{d}_k}) $ \hspace{1cm} $\triangleleft$
\item if $\vec{r}_{k+1}$ is sufficiently small ($||\vec{r}_{k+1}||_2/||\vec{r}_0||_2 <tol$), then exit loop
\item compute $\beta_k=({\color{carrotorange} \vec{r}_{k+1}^T \cdot \vec{r}_{k+1}})
/({\color{carrotorange} \vec{r}_k^T \cdot \vec{r}_k})$ \hspace{1cm} $\triangleleft$
\item ${\color{carrotorange} \vec{p}_{k+1}} = {\color{carrotorange} \vec{r}_{k+1} }+ \beta_k {\color{carrotorange}\vec{p}_k}$
\item $k=k+1$
\end{enumerate}
\item return $\vec{\cal V}_{k+1}$ and $\vec{\cal P}_{k+1}$ as result
\end{itemize}

The first residual $\vec{r}_0$ is then computed as follows:
\begin{lstlisting}
rvect_k=sps.linalg.spsolve(MP_mat,G_mat.T.dot(solV)-h_rhs) 
\end{lstlisting}
Then $\alpha_k$ is computed as follows:
\[
\alpha_k
= \frac{ \vec{r}_k^T \cdot  \vec{r}_k } { \tilde{\vec{p}}_k^T \cdot \vec{d}_k }
= \frac{ \vec{r}_k^T \cdot  \vec{r}_k } {(\G \cdot  \vec{p}_k)^T \cdot \vec{d}_k }
\quad
\Rightarrow
\quad
\alpha_k
= \frac{ \vec{r}_k^T \cdot {\bm M}_p \cdot \vec{r}_k } {\vec{p}_k^T \cdot {\bm H} \cdot \vec{d}_k }
\]
which translates into
\begin{lstlisting}
numerator=rvect_k.dot(MP_mat.dot(rvect_k))     
denominator=pvect_k.dot(H_mat.dot(dvect_k))   
alpha=numerator/denominator                  
\end{lstlisting}
The updated residual $\vec{r}_{k+1}$ follows. 
We can write step 6 
\[
\delta r\vec{r} = \vec{r}_{k+1} - \vec{r}_k = - \alpha_k (\G^T \cdot \vec{d}_k ) 
\]
which leads to
\begin{lstlisting}
dr=sps.linalg.spsolve(MP_mat,-alpha*G_mat.T.dot(dvect_k)) 
rvect_kp1=rvect_k+dr     
\end{lstlisting}
and finally $\beta_k$:
\[
\beta_k= \frac{ \vec{r}_{k+1}^T \cdot \vec{r}_{k+1} } { \vec{r}_k^T \cdot \vec{r}_k }
\quad
\Rightarrow
\quad
\beta_k= \frac{ \vec{r}_{k+1}^T \cdot {\bm M}_p \cdot \vec{r}_{k+1} }
{ \vec{r}_k^T \cdot {\bm M}_p \cdot \vec{r}_k }
\]
\begin{lstlisting}
numerator=rvect_kp1.dot(MP_mat.dot(rvect_kp1)) 
denominator=rvect_k.dot(MP_mat.dot(rvect_k))  
beta=numerator/denominator                   
\end{lstlisting}

In what follows solver 21 is identical to solver 4 (schur complement solver) but: 
different $\xi_{\cal V}=\|{\cal V}^{k+1}-{\cal V}\|_2$ and 
$\xi_{\cal P}=\|{\cal P}^{k+1}-{\cal P}^k\|_2$ calculation (convergence indicators) so as to 
compare with previous Uzawa 1 and 2 (and no preconditioner nor inner solver choice). 
Solver 22 is the Uzawa 3 solver (i.e solver 21) with $L_2$ projection.
I have also code solver 23, which is solver 22, albeit the inner solve 
is carried out with a LU solveri\footnote{\url{https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.splu.html}}: 
in practice the matrix $\K$ is then LU-decomposed once and the decomposition 
is then reused for each solve with $\K$. This results in a drastic
speedup as shown hereunder:

\begin{center}
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/L2/uzawa3/solver_convergence_P.pdf}
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/L2/uzawa3/solver_convergence_V.pdf} \\
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/L2/uzawa3/solver_convergence_alpha.pdf}
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/L2/uzawa3/time.pdf}\\
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/L2/uzawa3/errorsV.pdf}
\includegraphics[width=8cm]{python_codes/fieldstone_147/RESULTS/L2/uzawa3/errorsP.pdf}
\end{center}

Again, the L2 projection improves on the standard Uzawa solver, and in 
this particular case the number of iterations is divided by 3, and the total solve 
time is also more than halved.
The direct solver approach (solver 1) is also included in the plots as reference. 
We see that solver 1 still remains substantially faster.
Of course the performance of the Uzawa solvers is tied to the required tolerance,
which is set to a low value of $10^{-7}$.

I have here also explored the influence of the tolerance parameter. 
It was previously always $10^{-7}$ and I here document its effect on 
accuracy and solve time. 
We find that $10^{-5}$ (or even $10^{-4}$) seems to be sufficient (for this one isoviscous manufactured solution
and this range of resolutions). Since less iterations are needed it makes the iterative 
solvers faster, especially 21 since it 'spares' dozens of iterations. 
For solver 22 we find:

\begin{center}
\begin{tabular}{|c|cc|cc|cc|}
\hline
& tol=$10^{-4}$ & & tol=$10^{-5}$ & & tol=$10^{-7}$ & \\
nel   & time (s) & \# its &  time (s) & \# its &  time (s) & \# its  \\ 
\hline
1024  & 1.748    &  7  &2.046  &   8  & 2.073    & 10 \\
1600  & 4.159    &  8  &4.557  &   8  & 4.716    & 10 \\
2304  & 8.103    &  8  &8.358  &   9  &  9.374   & 11 \\
4096  & 18.581   &  8  &20.229 &   9  &  22.501  & 11 \\
6400  & 34.032   &  8  &36.868 &   9  &  41.397  & 11 \\
9216  & 73.158   &  9  &72.908 &   9  &  84.936  & 11 \\
16384 & 192.415  &  9  &205.503&   10 &  226.167 & 11 \\
25600 &          &     &       &      &  436.069 & 12 \\
36864 &          &     &       &      &  786.035 & 12 \\
\hline
\end{tabular}
\end{center}


One can also look at the time per solver iteration
\begin{center}
\includegraphics[width=10cm]{python_codes/fieldstone_147/RESULTS/L2/uzawa3/time_per_iteration.pdf}
\end{center}
We find that the additional solves and matrix-vector multiplications for the L2 variant (solver 22)
do not make each iteration substantially longer.

Since solver 23 relies on LU and therefore needs to store the $L$ and $U$
matrices, I expect it requires more memory than solver 22 - not proven yet.

